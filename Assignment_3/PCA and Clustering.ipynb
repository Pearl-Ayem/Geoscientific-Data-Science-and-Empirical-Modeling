{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from minisom import MiniSom\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.decomposition import PCA\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1: Following the guidelines below, perform hierarchical clustering of the data from data_problem1.mat (or data_problem1.csv) in the space of first few eigenvectors (decide yourself how many modes to use). Guidelines: \n",
    " \n",
    "> a) Perform PCA on the data (the data has 57 variables and 672 observations in time) and decide how many modes to keep. [1 point for correct PCA, 1 point for a reasonable choice of modes to keep]. \n",
    " \n",
    "> b) Perform hierarchical clustering with Ward's method on the data in the PC space of the modes you kept. Plot the dendrogram. [1 point for correct dendrogram]. \n",
    " \n",
    "> c) Chose three possible options for the optimal number of clusters (k) and plot the results (clustered data in PC space) for those options. [1 point for the correct choices of k, 1 point for the plots] \n",
    " \n",
    "> d) For only one of the cluster options above (on choice of k): plot, on the same graph, the mean pattern (57 variables) of each cluster (using the reconstructed data according to the selected number of PC modes). Plot the time-series (672 points) of occurrences of these clusters. [1 point for the mean patterns plot, 1 point for the time-series plot] \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data_problem1.csv') \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform PCA - I chose 3 modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86.30390747224028\n",
      "(57, 57)\n",
      "Expected sizes:\n",
      "\t57 eigenvectors, each of length 57\n",
      "\t57 eigenvalues, one for each eigenvector\n",
      "\t57 PCs, each of length 671\n",
      "Actual sizes:\n",
      "\t57 eigenvectors, each of length 57\n",
      "\t57 eigenvalues\n",
      "\t57 PCs, each of length 671\n"
     ]
    }
   ],
   "source": [
    "#normalize data and check it out\n",
    "data_norm = (df - df.mean())/df.std()\n",
    "## We want to run PCA\n",
    "n_modes = np.min(np.shape(data_norm))\n",
    "pca = PCA(n_components = n_modes)\n",
    "PCs = pca.fit_transform(data_norm)\n",
    "eigvecs = pca.components_\n",
    "fracVar = pca.explained_variance_ratio_\n",
    "n=3\n",
    "print(np.sum(fracVar[:n])*100)  #sum of the first n modes = total percent variance explained by the first neigvecs\n",
    "print(np.shape(eigvecs))\n",
    "#investigate: did PCA work as we expected?  What size of variables do we expect?\n",
    "nObservations = np.shape(data_norm)[0]\n",
    "nVariables = np.shape(data_norm)[1] \n",
    "print('Expected sizes:')\n",
    "print('\\t' + str(nVariables) + ' eigenvectors, each of length ' + str(nVariables))\n",
    "print('\\t' + str(nVariables) + ' eigenvalues, one for each eigenvector')\n",
    "print('\\t' + str(nVariables) + ' PCs, each of length ' + str(nObservations))\n",
    "print('Actual sizes:')\n",
    "print('\\t' + str(np.shape(eigvecs)[0]) + ' eigenvectors, each of length ' + str(np.shape(eigvecs)[1]))\n",
    "print('\\t' + str(len(fracVar)) + ' eigenvalues')\n",
    "print('\\t' + str(np.shape(PCs)[1]) + ' PCs, each of length ' + str(np.shape(PCs)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'frequency' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-1def0a285ea7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_norm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mlinked\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlinkage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'single'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mlabelList\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfrequency\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m7\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'frequency' is not defined"
     ]
    }
   ],
   "source": [
    "# data = np.copy(data_norm)\n",
    "# linked = linkage(data,'single')\n",
    "# labelList = range(1, len(frequency))\n",
    "\n",
    "# plt.figure(figsize=(10, 7))\n",
    "# dendrogram(linked, \n",
    "#            orientation='top', \n",
    "#            distance_sort='descending',\n",
    "#            truncate_mode='lastp',\n",
    "#            p=30)\n",
    "# plt.title('Dendrogram')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2: Following the guidelines below, perform clustering using self-organizing maps from data_problem2.mat (or data_problem2.csv).  The data is made up of normalized seasonal streamflow from 194 rivers in Alberta, Canada (i.e. there are 194 stations, each with 365 days of normalized streamflow).  The locations of each station are given by a latitude/longitude coordinate pair in stationLon.mat and stationLat.mat (or stationLon.csv and stationLat.csv). \n",
    " \n",
    "> a) Perform clustering using a 3 x 2 SOM.  Plot the 6 SOM patterns.  Plot the locations of the stations, coloured according to the cluster to which they belong. What is the frequency of each cluster?  [1 point for correct SOM patterns, 1 point for map of clusters, 1 point for correct frequencies] \n",
    " \n",
    "> b) Perform clustering a differently sized SOM, and plot the SOM patterns, locations of stations coloured by BMU, and frequency of each cluster as in a).  Discuss what you think are two key differences between your results from a) and b).  [1 point for plots, 2 points for discussion] \n",
    "\n",
    "> c) Calculate quantization error and topographic error for a range of SOM sizes (e.g.: 1x2, 2x2, 2x3, 3x3, 3x4, 4x4, 4x5, 5x5) and discuss what you find.  In what circumstance is it more important to minimize quantization error, versus in what circumstance is it more important to minimize topographic error? [1 point for discussion of QE and TE with map size, 1 point for discussion on circumstances to minimize QE/TE] \n",
    " \n",
    "> d) Calculate quantization error and topographic error for pairs of SOMs which have the same number of nodes but different map sizes and discuss what you find (e.g.: are QE and TE the same for a 2x3, 3x2 map, and 1x6 map?  A 3x4 and 4x3 map?  A 4x5 and 5x4 map? A 1x2 and 2x1 map?).  [1 point for identifying if QE/TE are the same for maps with the same number of nodes and different shape, 1 point for discussion]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
